import torch
import os
import json


def load_problem(name):
    """
    Load problem class by name
    """
    from problems.shopping.problem_shopping import Shopping
    
    if name == 'shopping':
        return Shopping
    else:
        raise ValueError(f"Unknown problem: {name}. Only 'shopping' is supported.")


def torch_load_cpu(load_path):
    return torch.load(load_path, map_location=lambda storage, loc: storage, weights_only=False)


def move_to(var, device):
    if isinstance(var, dict):
        return {k: move_to(v, device) for k, v in var.items()}
    return var.to(device)


def load_args(filename):
    with open(filename, 'r') as f:
        args = json.load(f)
    return args


def load_model(path, epoch=None):
    """Load trained model"""
    # from nets.attention_model import AttentionModel # ì´ importëŠ” ë§¨ ìœ„ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.

    if os.path.isfile(path):
        model_filename = path
        path = os.path.dirname(model_filename)
    elif os.path.isdir(path):
        if epoch is None:
            epoch = max(
                int(os.path.splitext(filename)[0].split("-")[1])
                for filename in os.listdir(path)
                if os.path.splitext(filename)[1] == '.pt'
            )
        model_filename = os.path.join(path, f'epoch-{epoch}.pt')
    else:
        raise ValueError(f"{path} is not a valid directory or file")

    args = load_args(os.path.join(path, 'args.json'))
    problem = load_problem(args['problem'])
    
    # ğŸ”‘ problem ê°ì²´ì— graph_sizeë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
    problem.size = args.get('graph_size')
    
    from nets.attention_model import AttentionModel
    # ğŸ”‘ AttentionModel í˜¸ì¶œ ì‹œ, ìƒˆë¡œìš´ GREAT ì•„í‚¤í…ì²˜ì— ë§ëŠ” ì¸ìë¡œ ë³€ê²½
    model = AttentionModel(
        embedding_dim=args['embedding_dim'],
        hidden_dim=args['hidden_dim'],
        problem=problem,
        n_encode_layers=args.get('n_encode_layers'),
        normalization=args.get('normalization'),
        n_heads=args.get('n_heads'),
        tanh_clipping=args.get('tanh_clipping'),
        nodeless=args.get('nodeless', False),
        dropout=args.get('dropout', 0.1)
    )

    load_data = torch_load_cpu(model_filename)
    model.load_state_dict({**model.state_dict(), **load_data.get('model', {})})
    model.eval()

    return model, args


# âŒ ì‚­ì œ: _load_model_file() í•¨ìˆ˜
# âŒ ì‚­ì œ: parse_softmax_temperature() í•¨ìˆ˜
# âŒ ì‚­ì œ: run_all_in_pool() í•¨ìˆ˜
# âŒ ì‚­ì œ: do_batch_rep() í•¨ìˆ˜
# âŒ ì‚­ì œ: sample_many() í•¨ìˆ˜