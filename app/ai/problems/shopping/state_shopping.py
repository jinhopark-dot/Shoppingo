import torch
from typing import NamedTuple
from ...utils.boolmask import mask_long2bool, mask_long_scatter


class StateShopping(NamedTuple):
    """
    μ‡Όν•‘ κ²½λ΅ μµμ ν™” λ¬Έμ μ μƒνƒ
    """
    
    # Fixed input
    dist_matrix: torch.Tensor  # (batch, n_nodes, n_nodes)
    start_idx: torch.Tensor    # (batch,)
    end_idx: torch.Tensor      # (batch,)
    
    # Beam searchμ© ID
    ids: torch.Tensor          # (batch, 1)
    
    # State
    prev_a: torch.Tensor       # (batch, 1) μ΄μ „ λ…Έλ“
    visited_: torch.Tensor     # (batch, 1, n_nodes) λ°©λ¬Έ λ§μ¤ν¬
    lengths: torch.Tensor      # (batch, 1) λ„μ  κ±°λ¦¬
    i: torch.Tensor            # (1,) μ¤ν… μΉ΄μ΄ν„°
    
    # β… 1. λ¶€λ¶„ κ²½λ΅ μ „μ²΄λ¥Ό μ €μ¥ν•  ν•„λ“ μ¶”κ°€
    tour_: torch.Tensor        # (batch, i) ν„μ¬κΉμ§€μ κ²½λ΅
    
    @property
    def visited(self):
        """λ°©λ¬Έ λ§μ¤ν¬ λ°ν™"""
        if self.visited_.dtype == torch.uint8:
            return self.visited_
        else:
            return mask_long2bool(self.visited_, n=self.dist_matrix.size(-2))
    
    def __getitem__(self, key):
        """Beam searchλ¥Ό μ„ν• μΈλ±μ‹±"""
        assert torch.is_tensor(key) or isinstance(key, slice)
        return self._replace(
            ids=self.ids[key],
            start_idx=self.start_idx[key],
            end_idx=self.end_idx[key],
            prev_a=self.prev_a[key],
            visited_=self.visited_[key],
            lengths=self.lengths[key],
            tour_=self.tour_[key] # β… getitemμ—λ„ μ¶”κ°€
        )
    
    @staticmethod
    def initialize(input, visited_dtype=torch.uint8):
        """
        μ΄κΈ° μƒνƒ μƒμ„±
        
        μ‹μ‘ λ…Έλ“λ¥Ό μ΄λ―Έ λ°©λ¬Έν• κ²ƒμΌλ΅ ν‘μ‹
        """
        dist_matrix = input['dist_matrix']
        batch_size, n_nodes, _ = dist_matrix.size()
        
        start_idx = input['start_idx']  # (batch,)
        end_idx = input['end_idx']      # (batch,)
        
        # μ‹μ‘ λ…Έλ“
        prev_a = start_idx.unsqueeze(-1)  # (batch, 1)
        tour_ = prev_a.clone()
        
        # λ°©λ¬Έ λ§μ¤ν¬ μ΄κΈ°ν™”
        if visited_dtype == torch.uint8:
            visited_ = torch.zeros(batch_size, 1, n_nodes, dtype=torch.uint8, device=dist_matrix.device)
        else:
            visited_ = torch.zeros(batch_size, 1, (n_nodes + 63) // 64, dtype=torch.int64, device=dist_matrix.device)
        
        # π†• μ‹μ‘ λ…Έλ“λ¥Ό λ°©λ¬Έν• κ²ƒμΌλ΅ ν‘μ‹
        if visited_dtype == torch.uint8:
            visited_ = visited_.scatter(-1, prev_a.unsqueeze(-1), 1)
        else:
            visited_ = mask_long_scatter(visited_, prev_a)
        
        return StateShopping(
            dist_matrix=dist_matrix,
            start_idx=start_idx,
            end_idx=end_idx,
            ids=torch.arange(batch_size, dtype=torch.int64, device=dist_matrix.device).unsqueeze(-1),
            prev_a=prev_a,
            visited_=visited_,
            lengths=torch.zeros(batch_size, 1, device=dist_matrix.device),
            i=torch.zeros(1, dtype=torch.int64, device=dist_matrix.device),
            tour_=tour_
        )
    
    def get_current_node(self):
        """ν„μ¬ λ…Έλ“ λ°ν™"""
        return self.prev_a
    
    def get_end_node(self):
        """λ©μ μ§€ λ…Έλ“ λ°ν™"""
        return self.end_idx.unsqueeze(-1)  # (batch, 1)
    
    def all_finished(self):
        """
        λ¨λ“  λ…Έλ“λ¥Ό λ°©λ¬Έν–λ”μ§€ ν™•μΈ
        """
        return self.i.item() >= self.dist_matrix.size(1) - 1
    
    def get_mask(self):
        """
        π”‘ ν•µμ‹¬: λ°©λ¬Έ λ¶κ°€λ¥ν• λ…Έλ“ λ§μ¤ν‚Ή
        
        κ·μΉ™:
        1. μ΄λ―Έ λ°©λ¬Έν• λ…Έλ“λ” λ°©λ¬Έ λ¶κ°€
        2. λ λ…Έλ“λ” λ¨λ“  λ‹¤λ¥Έ λ…Έλ“λ¥Ό λ°©λ¬Έν• ν›„μ—λ§ μ„ νƒ κ°€λ¥
        
        Returns:
            mask: (batch, 1, n_nodes) bool tensor
                  True = λ°©λ¬Έ λ¶κ°€, False = λ°©λ¬Έ κ°€λ¥
        """
        batch_size, _, n_nodes = self.visited.size()
        
        # 1. κΈ°λ³Έ λ§μ¤ν¬: μ΄λ―Έ λ°©λ¬Έν• λ…Έλ“
        mask = self.visited > 0  # (batch, 1, n_nodes)
        
        # 2. λ λ…Έλ“ νΉλ³„ μ²λ¦¬
        end_mask = self._get_end_node_mask()  # (batch, 1, n_nodes)
        
        # μµμΆ… λ§μ¤ν¬ = λ°©λ¬Έν• λ…Έλ“ OR λ λ…Έλ“ (μ΅°κ±΄λ¶€)
        mask = mask | end_mask
        
        return mask
    
    def _get_end_node_mask(self):
        """
        λ λ…Έλ“ λ§μ¤ν‚Ή λ΅μ§
        
        λ λ…Έλ“λ” λ§μ§€λ§‰μ—λ§ μ„ νƒ κ°€λ¥:
        - λ°©λ¬Έν•΄μ•Ό ν•  λ…Έλ“ μ = n_nodes - 1 (μ‹μ‘ λ…Έλ“ μ μ™Έ)
        - ν„μ¬ μ¤ν… < n_nodes - 1 μ΄λ©΄ λ λ…Έλ“ λ§μ¤ν‚Ή
        - ν„μ¬ μ¤ν… == n_nodes - 1 μ΄λ©΄ λ λ…Έλ“λ§ μ„ νƒ κ°€λ¥
        
        Returns:
            end_mask: (batch, 1, n_nodes) bool tensor
        """
        batch_size, _, n_nodes = self.visited.size()
        device = self.visited.device
        
        # λ λ…Έλ“ μ„μΉ λ§μ¤ν¬ μƒμ„±
        end_node_mask = torch.zeros(batch_size, 1, n_nodes, dtype=torch.bool, device=device)
        
        # κ° λ°°μΉλ³„λ΅ λ λ…Έλ“ μ„μΉ λ§ν‚Ή
        batch_idx = torch.arange(batch_size, device=device)
        end_node_mask[batch_idx, 0, self.end_idx] = True
        
        # π”‘ ν•µμ‹¬ λ΅μ§
        # μ•„μ§ λ¨λ“  λ…Έλ“λ¥Ό λ°©λ¬Έν•μ§€ μ•μ•μΌλ©΄ λ λ…Έλ“ λ§μ¤ν‚Ή
        # (n_nodes - 1: μ‹μ‘ λ…Έλ“ μ μ™Έ)
        current_step = self.i.item()
        
        if current_step < n_nodes - 2:
            # μ•„μ§ λ λ…Έλ“ μ„ νƒ λ¶κ°€
            return end_node_mask
        else:
            # λ§μ§€λ§‰ μ¤ν…: λ λ…Έλ“λ§ μ„ νƒ κ°€λ¥
            # λ”°λΌμ„ λ λ…Έλ“λ” λ§μ¤ν‚Ήν•μ§€ μ•μ
            return torch.zeros_like(end_node_mask)
    
    def update(self, selected):
        """
        μ„ νƒν• λ…Έλ“λ΅ μƒνƒ μ—…λ°μ΄νΈ
        """
        prev_a = selected.unsqueeze(-1)  # (batch, 1)
        
        # μ΄λ™ κ±°λ¦¬ κ³„μ‚°
        batch_size = self.dist_matrix.size(0)
        batch_idx = torch.arange(batch_size, device=self.dist_matrix.device)
        distances = self.dist_matrix[batch_idx, self.prev_a.squeeze(-1), selected]
        lengths = self.lengths + distances.unsqueeze(-1)
        
        # λ°©λ¬Έ λ§ν‚Ή
        if self.visited_.dtype == torch.uint8:
            visited_ = self.visited_.scatter(-1, prev_a.unsqueeze(-1), 1)
        else:
            visited_ = mask_long_scatter(self.visited_, prev_a)
        
        # β… 3. tour_ μ—…λ°μ΄νΈ (κΈ°μ΅΄ κ²½λ΅μ— μ„ νƒλ λ…Έλ“ μ¶”κ°€)
        tour_ = torch.cat((self.tour_, selected.unsqueeze(-1)), dim=1)

        return self._replace(
            prev_a=prev_a,
            visited_=visited_,
            lengths=lengths,
            i=self.i + 1,
            tour_=tour_ # tour_ μ—…λ°μ΄νΈ
        )
    
    def construct_solutions(self, actions):
        """
        ν–‰λ™ μ‹ν€€μ¤λ¥Ό μ†”λ£¨μ…μΌλ΅ λ³€ν™
        """
        return actions
    
    # state_shopping.pyμ— μ¶”κ°€

    def get_finished(self):
        """
        κ° μΈμ¤ν„΄μ¤κ°€ μ™„λ£λμ—λ”μ§€ λ°ν™
        
        Returns:
            finished: (batch,) 0 = μ§„ν–‰ μ¤‘, 1 = μ™„λ£
        """
        n_nodes = self.dist_matrix.size(1)
        finished = (self.i >= n_nodes - 1).long()
        return finished.expand(self.ids.size(0))